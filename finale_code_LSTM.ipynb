{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/cardioid/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "########## import libraries ############\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Bidirectional\n",
    "import IPython; from IPython.display import display, HTML\n",
    "def dfPrint(df):\n",
    "    display(HTML(df.to_html()))\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############# import data ##############\n",
    "# def import_data():\n",
    "#     train_path = \"../data/text_large_infy_mod.csv\"\n",
    "#     data = pd.read_csv(train_path)\n",
    "#     data.reset_index(inplace=True)\n",
    "#     data.drop('index', axis=1, inplace=True)\n",
    "#     data.drop(['year','month','date','hour','mins','no'], axis=1, inplace=True)\n",
    "#     print ('dataset loaded with shape', data.shape)\n",
    "#     return data\n",
    "\n",
    "# data = import_data()\n",
    "# dfPrint(data.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded with shape (1492, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>Google Alert - currency notes rbi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>Many countries with similar macroeconomic envi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>ICICI Bank's Chanda Kochhar, Axis Bank's Sikha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>Today Stock News – 01.03.2018</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>Sensex Opens Higher, Nifty Reclaims 10,600 | T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>Market Now: These stocks jump over 7% in a bea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>PNB fraud: Two rogue employees, one famed jewe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>Sensex And Nifty Upbeat Amid Global Rally, PNB...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>SBI raises retail FD rates, first time in thre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>Daiichi: Unpledged assets of Singh brothers to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>FinMin sets up new regulatory measures for big...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>INFO NIFTY</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>How to Buy Bitcoins?-For Asian Buyers Only</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Axis Bank — request to unfreeze my account</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>Goshoppingbazaar.com — fraud fake company indore</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>Aadhaar link Seeding Kolkata</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>Fortis Continues To Trade In The Red</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>Closing Bell: Sensex ends off day's high; Midc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>Panasonic P100 now available on Snapdeal for R...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>FAUJI INDIA MARCH 2018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " ############ import data ##############\n",
    "def import_data():\n",
    "    train_path = \"../data/AXISBANK.csv\"\n",
    "    data = pd.read_csv(train_path)\n",
    "#     data.reset_index(inplace=True)\n",
    "#     data.drop('index', axis=1, inplace=True)\n",
    "    data.drop(['time','index'], axis=1, inplace=True)\n",
    "    print ('dataset loaded with shape', data.shape)\n",
    "    return data\n",
    "\n",
    "data = import_data()\n",
    "quantity = 1\n",
    "dfPrint(data.sample(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_decription(line):\n",
    "    words = list(set(line.split()))\n",
    "    words = list(sorted([x.lower() for x in words]))\n",
    "#     print (len(words))\n",
    "#     print (words)\n",
    "    words2 = words.copy()\n",
    "    for word in list(set(words2).intersection(stop_words)):\n",
    "        words.remove(word)\n",
    "#     out = ' '.join(words)\n",
    "    out = ' '.join(e for e in words if e.isalnum())\n",
    "    if out == \"\":\n",
    "        print(words)\n",
    "#     table = str.maketrans({key: None for key in string.punctuation})\n",
    "#     out.translate(table, string.punctuation)\n",
    "    return out\n",
    "data[\"title\"] = data[\"title\"].map(clean_decription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1247    banking indian lender news pnb slides stories ...\n",
      "229                        account bank reflecting refund\n",
      "665                                         atm cash indi\n",
      "38                           axis bank pay start whatsapp\n",
      "885     cbi details five nostro pnb seeks transactions...\n",
      "Name: title, dtype: object\n",
      "2328\n"
     ]
    }
   ],
   "source": [
    "############### define documents ################\n",
    "docs = data[\"title\"]+\n",
    "############# define class labels ################\n",
    "labels = data[\"label\"]\n",
    "############## prepare tokenizer #################\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "########## integer encode the documents ##########\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "########## pad documents to a max length ##########\n",
    "max_length = 100\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "\n",
    "print(docs.sample(5))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 399851 word vectors.\n"
     ]
    }
   ],
   "source": [
    "### load the whole embedding into memory ############\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B.300d.txt')\n",
    "for num, line in enumerate(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    if word in stop_words :\n",
    "#         print (word)\n",
    "        continue\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "    \n",
    "f.close()\n",
    "max_width = 300\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2148\n"
     ]
    }
   ],
   "source": [
    "########### create a weight matrix for words in training docs ############\n",
    "embedding_matrix = zeros((vocab_size, max_width))\n",
    "mil=0\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        mil+=1\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(mil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################ define model#################\n",
    "# model = Sequential()\n",
    "# e = Embedding(vocab_size, max_width, weights=[embedding_matrix], input_length=max_length, trainable=True)\n",
    "# model.add(e)\n",
    "# model.add(LSTM(35,return_sequences=False))\n",
    "# # model.add(GlobalMaxPooling1D(pool_size=2, strides=None, padding='valid'))\n",
    "# # model.add(Flatten())\n",
    "# model.add(Dense(50,  kernel_initializer=\"normal\",activation = 'relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(25,  kernel_initializer=\"normal\",activation = 'relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(15,  kernel_initializer=\"normal\",activation = 'relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# ############## compile the model ##############\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc','binary_accuracy'])\n",
    "# ############# summarize the model ##############\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 300)          698400    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 70)           94080     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 70)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 15)                1065      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 16        \n",
      "=================================================================\n",
      "Total params: 793,561\n",
      "Trainable params: 793,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "################ define model#################\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, max_width, weights=[embedding_matrix], input_length=max_length, trainable=True)\n",
    "model.add(e)\n",
    "model.add(Bidirectional(LSTM(35, return_sequences=True)))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "# model.add(Bidirectional(LSTM(10)))\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(15,  kernel_initializer=\"normal\",activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "############## compile the model ##############\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc','binary_accuracy'])\n",
    "############# summarize the model ##############\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1073 samples, validate on 120 samples\n",
      "Epoch 1/15\n",
      "1073/1073 [==============================] - 8s 7ms/step - loss: 0.6920 - acc: 0.5349 - binary_accuracy: 0.5349 - val_loss: 0.6809 - val_acc: 0.5833 - val_binary_accuracy: 0.5833\n",
      "Epoch 2/15\n",
      "1073/1073 [==============================] - 6s 6ms/step - loss: 0.6778 - acc: 0.5722 - binary_accuracy: 0.5722 - val_loss: 0.6744 - val_acc: 0.6167 - val_binary_accuracy: 0.6167\n",
      "Epoch 3/15\n",
      "1073/1073 [==============================] - 7s 6ms/step - loss: 0.6610 - acc: 0.6076 - binary_accuracy: 0.6076 - val_loss: 0.6568 - val_acc: 0.6500 - val_binary_accuracy: 0.6500\n",
      "Epoch 4/15\n",
      "1073/1073 [==============================] - 6s 6ms/step - loss: 0.5933 - acc: 0.6943 - binary_accuracy: 0.6943 - val_loss: 0.6305 - val_acc: 0.6417 - val_binary_accuracy: 0.6417\n",
      "Epoch 5/15\n",
      "1073/1073 [==============================] - 6s 6ms/step - loss: 0.4970 - acc: 0.7642 - binary_accuracy: 0.7642 - val_loss: 0.6018 - val_acc: 0.6750 - val_binary_accuracy: 0.6750\n",
      "Epoch 6/15\n",
      "1073/1073 [==============================] - 7s 6ms/step - loss: 0.3585 - acc: 0.8500 - binary_accuracy: 0.8500 - val_loss: 0.6567 - val_acc: 0.6667 - val_binary_accuracy: 0.6667\n",
      "Epoch 7/15\n",
      "1073/1073 [==============================] - 6s 6ms/step - loss: 0.3064 - acc: 0.8826 - binary_accuracy: 0.8826 - val_loss: 0.6436 - val_acc: 0.6833 - val_binary_accuracy: 0.6833\n",
      "Epoch 8/15\n",
      "1073/1073 [==============================] - 6s 6ms/step - loss: 0.2294 - acc: 0.9282 - binary_accuracy: 0.9282 - val_loss: 0.7458 - val_acc: 0.6833 - val_binary_accuracy: 0.6833\n",
      "Epoch 9/15\n",
      "1073/1073 [==============================] - 6s 6ms/step - loss: 0.1934 - acc: 0.9320 - binary_accuracy: 0.9320 - val_loss: 0.8306 - val_acc: 0.6500 - val_binary_accuracy: 0.6500\n",
      "Epoch 10/15\n",
      "1073/1073 [==============================] - 7s 6ms/step - loss: 0.1540 - acc: 0.9432 - binary_accuracy: 0.9432 - val_loss: 0.9239 - val_acc: 0.6833 - val_binary_accuracy: 0.6833\n",
      "Epoch 11/15\n",
      "1073/1073 [==============================] - 7s 7ms/step - loss: 0.1614 - acc: 0.9376 - binary_accuracy: 0.9376 - val_loss: 0.9639 - val_acc: 0.6500 - val_binary_accuracy: 0.6500\n",
      "Epoch 12/15\n",
      "1073/1073 [==============================] - 7s 6ms/step - loss: 0.1532 - acc: 0.9441 - binary_accuracy: 0.9441 - val_loss: 0.9847 - val_acc: 0.6583 - val_binary_accuracy: 0.6583\n",
      "Epoch 13/15\n",
      "1073/1073 [==============================] - 7s 7ms/step - loss: 0.1449 - acc: 0.9506 - binary_accuracy: 0.9506 - val_loss: 1.0004 - val_acc: 0.6583 - val_binary_accuracy: 0.6583\n",
      "Epoch 14/15\n",
      "1073/1073 [==============================] - 7s 7ms/step - loss: 0.1259 - acc: 0.9562 - binary_accuracy: 0.9562 - val_loss: 1.0685 - val_acc: 0.6583 - val_binary_accuracy: 0.6583\n",
      "Epoch 15/15\n",
      "1073/1073 [==============================] - 7s 7ms/step - loss: 0.1255 - acc: 0.9590 - binary_accuracy: 0.9590 - val_loss: 1.1204 - val_acc: 0.6667 - val_binary_accuracy: 0.6667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc3ec3f4b70>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### split into train test data #################\n",
    "padded_docs_train, padded_docs_test, labels_train, labels_test = train_test_split(padded_docs, labels, test_size=0.2, random_state=69)\n",
    "################### fit the model ##########3#############\n",
    "model.fit(padded_docs_train, labels_train, epochs=15, validation_split = 0.1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299/299 [==============================] - 1s 2ms/step\n",
      "Accuracy: 67.558528\n",
      "binaryAccuracy: 67.558528\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy,binaryAccuracy = model.evaluate(padded_docs_test, labels_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "print('binaryAccuracy: %f' % (binaryAccuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "predict_path = \"../data/AXISBANKTest.csv\"\n",
    "test = pd.read_csv(predict_path)\n",
    "############## prepare tokenizer #################\n",
    "t1 = Tokenizer()\n",
    "t1.fit_on_texts(test)\n",
    "\n",
    "# predict_path = \"../data/AXISBANKTest.csv\"\n",
    "# test = pd.read_csv(predict_path)\n",
    "print(test.size)\n",
    "########## integer encode the documents ##########\n",
    "encoded_test = t.texts_to_sequences(test['title'])\n",
    "print(len(encoded_test))\n",
    "########## pad documents to a max length ##########\n",
    "max_length = 100\n",
    "padded_test = pad_sequences(encoded_test, maxlen=max_length, padding='post')\n",
    "\n",
    "predicted = model.predict(padded_test)\n",
    "# for i in range(predicted.size):\n",
    "#     if predicted[i] < 0.5 :\n",
    "#         predicted[i] = 0.0\n",
    "#     else :\n",
    "#         predicted[i] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy\n",
      "confidance is : 0.631579\n"
     ]
    }
   ],
   "source": [
    "confidance = np.mean(predicted)\n",
    "if confidance > 0.5:\n",
    "    print (\"buy\")\n",
    "    print(\"confidance is :\",confidance)\n",
    "    \n",
    "else:\n",
    "    if quantity != 0 :\n",
    "        print (\"sell\")\n",
    "        print(\"confidance is :\",1-confidance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
