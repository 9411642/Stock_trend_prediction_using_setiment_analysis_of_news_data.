{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/cardioid/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "########## import libraries ############\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "import IPython; from IPython.display import display, HTML\n",
    "def dfPrint(df):\n",
    "    display(HTML(df.to_html()))\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############# import data ##############\n",
    "# def import_data():\n",
    "#     train_path = \"../data/text_large_infy_mod.csv\"\n",
    "#     data = pd.read_csv(train_path)\n",
    "#     data.reset_index(inplace=True)\n",
    "#     data.drop('index', axis=1, inplace=True)\n",
    "#     data.drop(['year','month','date','hour','mins','no'], axis=1, inplace=True)\n",
    "#     print ('dataset loaded with shape', data.shape)\n",
    "#     return data\n",
    "\n",
    "# data = import_data()\n",
    "# dfPrint(data.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded with shape (1492, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>BSE's transaction charge waiver on Sensex stoc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>Timeline: Developments in the $2 billion Punja...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>News in Numbers: 31 banks gave working capital...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>Google Alert - NON PERFORMING ASSETS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Nifty | Stock Market Now: Nifty above 10,450, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Ways2Capital Closing Update : Sensex Ends Off ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>Google Alert - Bank</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>Federal Bank Share Price: Market Now: Private ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>Sensex slips 144 points ahead of F&amp;O expiry</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>Google Alert - income tax india</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>Sensex rises 130 points, Nifty above 10,400, I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1310</th>\n",
       "      <td>EarlySalary and Niki.ai Introduce New Shopping...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>Grab Now - Buy 1 Get 2 FREE On Bombay Dyeing B...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>PNB fraud favouring Nirav Modi started in 2008...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Uninvoked guarantees put lenders, investors in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>DNA SPECIAL | Finance sleuths raised red flag ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>Snapdeal sets up Women's Day store</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>Sensex loses 162 pts, Nifty settles below 10,5...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>Axis Bank â€” behaviour of sakchi and bistupur e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>SFIO: PNB fraud fallout: 107 firms, 7 LLPs und...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "############ import data ##############\n",
    "def import_data():\n",
    "    train_path = \"../data/AXISBANK.csv\"\n",
    "    data = pd.read_csv(train_path)\n",
    "#     data.reset_index(inplace=True)\n",
    "#     data.drop('index', axis=1, inplace=True)\n",
    "    data.drop(['time','index'], axis=1, inplace=True)\n",
    "    print ('dataset loaded with shape', data.shape)\n",
    "    return data\n",
    "\n",
    "data = import_data()\n",
    "dfPrint(data.sample(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_decription(line):\n",
    "    words = list(set(line.split()))\n",
    "    words = list(sorted([x.lower() for x in words]))\n",
    "#     print (len(words))\n",
    "#     print (words)\n",
    "    words2 = words.copy()\n",
    "    for word in list(set(words2).intersection(stop_words)):\n",
    "        words.remove(word)\n",
    "#     out = ' '.join(words)\n",
    "    out = ' '.join(e for e in words if e.isalnum())\n",
    "    if out == \"\":\n",
    "        print(words)\n",
    "#     table = str.maketrans({key: None for key in string.punctuation})\n",
    "#     out.translate(table, string.punctuation)\n",
    "    return out\n",
    "data[\"title\"] = data[\"title\"].map(clean_decription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1033                           claim cpm exposes pnb scam\n",
      "174     asks bata india leak probe quarterly results s...\n",
      "285     11999 32 6 drop flipkart gb honor hot online p...\n",
      "1171                           gainers losers session top\n",
      "1228    agencies banks branches indian nirav overseas ...\n",
      "Name: title, dtype: object\n",
      "2328\n"
     ]
    }
   ],
   "source": [
    "############### define documents ################\n",
    "docs = data[\"title\"]\n",
    "############# define class labels ################\n",
    "labels = data[\"label\"]\n",
    "############## prepare tokenizer #################\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "########## integer encode the documents ##########\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "########## pad documents to a max length ##########\n",
    "max_length = 100\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "\n",
    "print(docs.sample(5))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 399851 word vectors.\n"
     ]
    }
   ],
   "source": [
    "########### load the whole embedding into memory ############\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B.300d.txt')\n",
    "for num, line in enumerate(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    if word in stop_words :\n",
    "#         print (word)\n",
    "        continue\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "    \n",
    "f.close()\n",
    "max_width = 300\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2148\n"
     ]
    }
   ],
   "source": [
    "########### create a weight matrix for words in training docs ############\n",
    "embedding_matrix = zeros((vocab_size, max_width))\n",
    "mil=0\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        mil+=1\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(mil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 100, 300)          698400    \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 98, 15)            13515     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1470)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 15)                22065     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 16        \n",
      "=================================================================\n",
      "Total params: 733,996\n",
      "Trainable params: 733,996\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "################ define model#################\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, max_width, weights=[embedding_matrix], input_length=max_length, trainable=True )\n",
    "model.add(e)\n",
    "model.add(Conv1D(filters= 15 ,kernel_size= 3 ,activation=\"relu\",strides=1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(15,  kernel_initializer=\"normal\",activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "############## compile the model ##############\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc','binary_accuracy'])\n",
    "############# summarize the model ##############\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 894 samples, validate on 299 samples\n",
      "Epoch 1/25\n",
      "894/894 [==============================] - 2s 3ms/step - loss: 0.6898 - acc: 0.5324 - binary_accuracy: 0.5324 - val_loss: 0.6826 - val_acc: 0.5886 - val_binary_accuracy: 0.5886\n",
      "Epoch 2/25\n",
      "894/894 [==============================] - 1s 904us/step - loss: 0.6350 - acc: 0.6409 - binary_accuracy: 0.6409 - val_loss: 0.6747 - val_acc: 0.6054 - val_binary_accuracy: 0.6054\n",
      "Epoch 3/25\n",
      "894/894 [==============================] - 1s 913us/step - loss: 0.5545 - acc: 0.7483 - binary_accuracy: 0.7483 - val_loss: 0.6643 - val_acc: 0.5886 - val_binary_accuracy: 0.5886\n",
      "Epoch 4/25\n",
      "894/894 [==============================] - 1s 921us/step - loss: 0.4377 - acc: 0.8289 - binary_accuracy: 0.8289 - val_loss: 0.6779 - val_acc: 0.6087 - val_binary_accuracy: 0.6087\n",
      "Epoch 5/25\n",
      "894/894 [==============================] - 1s 903us/step - loss: 0.3058 - acc: 0.9038 - binary_accuracy: 0.9038 - val_loss: 0.7343 - val_acc: 0.5953 - val_binary_accuracy: 0.5953\n",
      "Epoch 6/25\n",
      "894/894 [==============================] - 1s 912us/step - loss: 0.2270 - acc: 0.9284 - binary_accuracy: 0.9284 - val_loss: 0.7411 - val_acc: 0.6154 - val_binary_accuracy: 0.6154\n",
      "Epoch 7/25\n",
      "894/894 [==============================] - 1s 909us/step - loss: 0.1690 - acc: 0.9474 - binary_accuracy: 0.9474 - val_loss: 0.7709 - val_acc: 0.6187 - val_binary_accuracy: 0.6187\n",
      "Epoch 8/25\n",
      "894/894 [==============================] - 1s 908us/step - loss: 0.1401 - acc: 0.9553 - binary_accuracy: 0.9553 - val_loss: 0.8421 - val_acc: 0.6355 - val_binary_accuracy: 0.6355\n",
      "Epoch 9/25\n",
      "894/894 [==============================] - 1s 911us/step - loss: 0.1284 - acc: 0.9586 - binary_accuracy: 0.9586 - val_loss: 0.8530 - val_acc: 0.6622 - val_binary_accuracy: 0.6622\n",
      "Epoch 10/25\n",
      "894/894 [==============================] - 1s 903us/step - loss: 0.1167 - acc: 0.9586 - binary_accuracy: 0.9586 - val_loss: 0.8821 - val_acc: 0.6355 - val_binary_accuracy: 0.6355\n",
      "Epoch 11/25\n",
      "894/894 [==============================] - 1s 899us/step - loss: 0.1007 - acc: 0.9631 - binary_accuracy: 0.9631 - val_loss: 0.9556 - val_acc: 0.6522 - val_binary_accuracy: 0.6522\n",
      "Epoch 12/25\n",
      "894/894 [==============================] - 1s 904us/step - loss: 0.0995 - acc: 0.9631 - binary_accuracy: 0.9631 - val_loss: 0.9296 - val_acc: 0.6455 - val_binary_accuracy: 0.6455\n",
      "Epoch 13/25\n",
      "894/894 [==============================] - 1s 913us/step - loss: 0.0964 - acc: 0.9620 - binary_accuracy: 0.9620 - val_loss: 0.9701 - val_acc: 0.6355 - val_binary_accuracy: 0.6355\n",
      "Epoch 14/25\n",
      "894/894 [==============================] - 1s 917us/step - loss: 0.0914 - acc: 0.9642 - binary_accuracy: 0.9642 - val_loss: 0.9551 - val_acc: 0.6589 - val_binary_accuracy: 0.6589\n",
      "Epoch 15/25\n",
      "894/894 [==============================] - 1s 909us/step - loss: 0.0772 - acc: 0.9642 - binary_accuracy: 0.9642 - val_loss: 0.9971 - val_acc: 0.6722 - val_binary_accuracy: 0.6722\n",
      "Epoch 16/25\n",
      "894/894 [==============================] - 1s 924us/step - loss: 0.0774 - acc: 0.9664 - binary_accuracy: 0.9664 - val_loss: 0.9890 - val_acc: 0.6656 - val_binary_accuracy: 0.6656\n",
      "Epoch 17/25\n",
      "894/894 [==============================] - 1s 917us/step - loss: 0.0677 - acc: 0.9664 - binary_accuracy: 0.9664 - val_loss: 1.0532 - val_acc: 0.6689 - val_binary_accuracy: 0.6689\n",
      "Epoch 18/25\n",
      "894/894 [==============================] - 1s 910us/step - loss: 0.0709 - acc: 0.9709 - binary_accuracy: 0.9709 - val_loss: 1.0774 - val_acc: 0.6923 - val_binary_accuracy: 0.6923\n",
      "Epoch 19/25\n",
      "894/894 [==============================] - 1s 915us/step - loss: 0.0786 - acc: 0.9631 - binary_accuracy: 0.9631 - val_loss: 1.0473 - val_acc: 0.6589 - val_binary_accuracy: 0.6589\n",
      "Epoch 20/25\n",
      "894/894 [==============================] - 1s 910us/step - loss: 0.0747 - acc: 0.9631 - binary_accuracy: 0.9631 - val_loss: 1.0177 - val_acc: 0.6756 - val_binary_accuracy: 0.6756\n",
      "Epoch 21/25\n",
      "894/894 [==============================] - 1s 900us/step - loss: 0.0699 - acc: 0.9653 - binary_accuracy: 0.9653 - val_loss: 1.0779 - val_acc: 0.6555 - val_binary_accuracy: 0.6555\n",
      "Epoch 22/25\n",
      "894/894 [==============================] - 1s 909us/step - loss: 0.0611 - acc: 0.9687 - binary_accuracy: 0.9687 - val_loss: 1.1380 - val_acc: 0.6689 - val_binary_accuracy: 0.6689\n",
      "Epoch 23/25\n",
      "894/894 [==============================] - 1s 911us/step - loss: 0.0674 - acc: 0.9664 - binary_accuracy: 0.9664 - val_loss: 1.0960 - val_acc: 0.6656 - val_binary_accuracy: 0.6656\n",
      "Epoch 24/25\n",
      "894/894 [==============================] - 1s 902us/step - loss: 0.0651 - acc: 0.9687 - binary_accuracy: 0.9687 - val_loss: 1.1319 - val_acc: 0.6522 - val_binary_accuracy: 0.6522\n",
      "Epoch 25/25\n",
      "894/894 [==============================] - 1s 894us/step - loss: 0.0609 - acc: 0.9653 - binary_accuracy: 0.9653 - val_loss: 1.1230 - val_acc: 0.6656 - val_binary_accuracy: 0.6656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb306fb7358>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############## split into train test data ##############\n",
    "padded_docs_train, padded_docs_test, labels_train, labels_test = train_test_split(padded_docs, labels, test_size=0.2, random_state=69)\n",
    "################### fit the model #####################\n",
    "model.fit(padded_docs_train, labels_train, epochs=25, validation_split = 0.25,shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299/299 [==============================] - 0s 270us/step\n",
      "Accuracy: 58.528428\n",
      "binaryAccuracy: 58.528428\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy,binaryAccuracy = model.evaluate(padded_docs_test, labels_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "print('binaryAccuracy: %f' % (binaryAccuracy*100))\n",
    "predicted = model.predict(padded_docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_path = \"../data/AXISBANKTest.csv\"\n",
    "test = pd.read_csv(predict_path)\n",
    "############## prepare tokenizer #################\n",
    "t1 = Tokenizer()\n",
    "t1.fit_on_texts(test)\n",
    "\n",
    "# predict_path = \"../data/AXISBANKTest.csv\"\n",
    "# test = pd.read_csv(predict_path)\n",
    "print(test.size)\n",
    "########## integer encode the documents ##########\n",
    "encoded_test = t.texts_to_sequences(test['title'])\n",
    "print(len(encoded_test))\n",
    "########## pad documents to a max length ##########\n",
    "max_length = 100\n",
    "padded_test = pad_sequences(encoded_test, maxlen=max_length, padding='post')\n",
    "\n",
    "predicted = model.predict(padded_test)\n",
    "# for i in range(predicted.size):\n",
    "#     if predicted[i] < 0.5 :\n",
    "#         predicted[i] = 0.0\n",
    "#     else :\n",
    "#         predicted[i] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidance = np.mean(predicted)\n",
    "if confidance > 0.5:\n",
    "    print (\"buy\")\n",
    "    print(\"confidance is :\",confidance)\n",
    "    \n",
    "else:\n",
    "    if quantity != 0 :\n",
    "        print (\"sell\")\n",
    "        print(\"confidance is :\",1-confidance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
