{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/cardioid/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "########## import libraries ############\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "import IPython; from IPython.display import display, HTML\n",
    "def dfPrint(df):\n",
    "    display(HTML(df.to_html()))\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded with shape (1622, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>Google Alert - NSE Options and future</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>Market Now: BSE Consumer Durables index in the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>sensex today: Markets surge even as bank stock...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>eMudhra looks to expand beyond India</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>Google Alert - Infosys</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ############# import data ##############\n",
    "# def import_data():\n",
    "#     train_path = \"../data/text_large_infy_mod.csv\"\n",
    "#     data = pd.read_csv(train_path)\n",
    "#     data.reset_index(inplace=True)\n",
    "#     data.drop('index', axis=1, inplace=True)\n",
    "#     data.drop(['year','month','date','hour','mins','no'], axis=1, inplace=True)\n",
    "#     print ('dataset loaded with shape', data.shape)\n",
    "#     return data\n",
    "\n",
    "# data = import_data()\n",
    "# dfPrint(data.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded with shape (1492, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Yes Bank acquires 17.31% stake in Fortis Healt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>Google Alert - BSE NSE india</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>How believable is PNB's version of the fraud?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>Google Alert - rbi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>RBI slaps Rs 3 crore penalty on Axis Bank, Rs ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>creer cv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>Axis Bank — behaviovr of sakchi and bistupur e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>Sensex Opens Higher, Nifty Reclaims 10,600 | T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>Sensex: Top performing multicap funds busy shu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>Airtel — calling a toll free number declared b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>MARKETS LIVE: Sensex opens sharply up, Nifty r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>Market Now: PNB, BoB, SBI among most traded st...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>Nearly 40 stocks hit fresh 52-week lows on NSE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>Google Alert - whatsapp</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Live Commodity News &amp; Tips | Open market, Sens...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>Sensex recovers 157 points, Rupee falls 31 pai...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>PNB fraud: PNB Fraud: RBI sets up panel, asks ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>Nifty extends its slump on negative global cue...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>ICICI's Chanda Kocchar, Axis Bank's Shikha Sha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>Paisabazaar.com Opens 2 Lakh Savings Accounts ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "############ import data ##############\n",
    "def import_data():\n",
    "    train_path = \"../data/AXISBANK.csv\"\n",
    "    data = pd.read_csv(train_path)\n",
    "#     data.reset_index(inplace=True)\n",
    "#     data.drop('index', axis=1, inplace=True)\n",
    "    data.drop(['time','index'], axis=1, inplace=True)\n",
    "    print ('dataset loaded with shape', data.shape)\n",
    "    return data\n",
    "\n",
    "data = import_data()\n",
    "dfPrint(data.sample(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_decription(line):\n",
    "    words = list(set(line.split()))\n",
    "    words = list(sorted([x.lower() for x in words]))\n",
    "#     print (len(words))\n",
    "#     print (words)\n",
    "    words2 = words.copy()\n",
    "    for word in list(set(words2).intersection(stop_words)):\n",
    "        words.remove(word)\n",
    "#     out = ' '.join(words)\n",
    "    out = ' '.join(e for e in words if e.isalnum())\n",
    "    if out == \"\":\n",
    "        print(words)\n",
    "#     table = str.maketrans({key: None for key in string.punctuation})\n",
    "#     out.translate(table, string.punctuation)\n",
    "    return out\n",
    "data[\"title\"] = data[\"title\"].map(clean_decription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "803                      money phonepe send send whatsapp\n",
      "921                                      news stock today\n",
      "1074    approved current declare languages mandarin mo...\n",
      "780     carnage crore double fraud hits low pnb rises ...\n",
      "80                                            bank rising\n",
      "Name: title, dtype: object\n",
      "2328\n"
     ]
    }
   ],
   "source": [
    "############### define documents ################\n",
    "docs = data[\"title\"]\n",
    "############# define class labels ################\n",
    "labels = data[\"label\"]\n",
    "############## prepare tokenizer #################\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "########## integer encode the documents ##########\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "########## pad documents to a max length ##########\n",
    "max_length = 100\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "\n",
    "print(docs.sample(5))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 399851 word vectors.\n"
     ]
    }
   ],
   "source": [
    "########### load the whole embedding into memory ############\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B.300d.txt')\n",
    "for num, line in enumerate(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    if word in stop_words :\n",
    "#         print (word)\n",
    "        continue\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "    \n",
    "f.close()\n",
    "max_width = 300\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2148\n"
     ]
    }
   ],
   "source": [
    "########### create a weight matrix for words in training docs ############\n",
    "embedding_matrix = zeros((vocab_size, max_width))\n",
    "mil=0\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        mil+=1\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(mil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 300)          698400    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 30000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 150)               4500150   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               15100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 15)                390       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 16        \n",
      "=================================================================\n",
      "Total params: 5,220,381\n",
      "Trainable params: 5,220,381\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "################ define model#################\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, max_width, weights=[embedding_matrix], input_length=max_length, trainable=True)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(150,  kernel_initializer=\"normal\",activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(100,  kernel_initializer=\"normal\",activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(50,  kernel_initializer=\"normal\",activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(25,  kernel_initializer=\"normal\",activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(15,  kernel_initializer=\"normal\",activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "############## compile the model ##############\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc','binary_accuracy'])\n",
    "############# summarize the model ##############\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1073 samples, validate on 120 samples\n",
      "Epoch 1/15\n",
      "1073/1073 [==============================] - 3s 3ms/step - loss: 0.4279 - acc: 0.8322 - binary_accuracy: 0.8322 - val_loss: 0.7202 - val_acc: 0.5917 - val_binary_accuracy: 0.5917\n",
      "Epoch 2/15\n",
      "1073/1073 [==============================] - 3s 3ms/step - loss: 0.2445 - acc: 0.9152 - binary_accuracy: 0.9152 - val_loss: 0.7540 - val_acc: 0.6167 - val_binary_accuracy: 0.6167\n",
      "Epoch 3/15\n",
      "1073/1073 [==============================] - 3s 3ms/step - loss: 0.1553 - acc: 0.9487 - binary_accuracy: 0.9487 - val_loss: 1.0591 - val_acc: 0.6250 - val_binary_accuracy: 0.6250\n",
      "Epoch 4/15\n",
      "1073/1073 [==============================] - 3s 3ms/step - loss: 0.1332 - acc: 0.9534 - binary_accuracy: 0.9534 - val_loss: 1.1138 - val_acc: 0.5917 - val_binary_accuracy: 0.5917\n",
      "Epoch 5/15\n",
      "1073/1073 [==============================] - 3s 3ms/step - loss: 0.1203 - acc: 0.9543 - binary_accuracy: 0.9543 - val_loss: 1.1273 - val_acc: 0.6500 - val_binary_accuracy: 0.6500\n",
      "Epoch 6/15\n",
      "1073/1073 [==============================] - 3s 3ms/step - loss: 0.1268 - acc: 0.9497 - binary_accuracy: 0.9497 - val_loss: 1.0991 - val_acc: 0.6667 - val_binary_accuracy: 0.6667\n",
      "Epoch 7/15\n",
      "1073/1073 [==============================] - 3s 3ms/step - loss: 0.1028 - acc: 0.9618 - binary_accuracy: 0.9618 - val_loss: 1.1695 - val_acc: 0.6500 - val_binary_accuracy: 0.6500\n",
      "Epoch 8/15\n",
      "1073/1073 [==============================] - 3s 3ms/step - loss: 0.0853 - acc: 0.9618 - binary_accuracy: 0.9618 - val_loss: 1.3978 - val_acc: 0.6417 - val_binary_accuracy: 0.6417\n",
      "Epoch 9/15\n",
      "1073/1073 [==============================] - 3s 3ms/step - loss: 0.0948 - acc: 0.9637 - binary_accuracy: 0.9637 - val_loss: 1.3842 - val_acc: 0.6250 - val_binary_accuracy: 0.6250\n",
      "Epoch 10/15\n",
      "1073/1073 [==============================] - 3s 3ms/step - loss: 0.0771 - acc: 0.9637 - binary_accuracy: 0.9637 - val_loss: 1.6521 - val_acc: 0.6167 - val_binary_accuracy: 0.6167\n",
      "Epoch 11/15\n",
      "1073/1073 [==============================] - 3s 3ms/step - loss: 0.0998 - acc: 0.9627 - binary_accuracy: 0.9627 - val_loss: 1.6503 - val_acc: 0.5917 - val_binary_accuracy: 0.5917\n",
      "Epoch 12/15\n",
      "1073/1073 [==============================] - 3s 3ms/step - loss: 0.0868 - acc: 0.9618 - binary_accuracy: 0.9618 - val_loss: 1.7265 - val_acc: 0.6167 - val_binary_accuracy: 0.6167\n",
      "Epoch 13/15\n",
      "1073/1073 [==============================] - 3s 3ms/step - loss: 0.0834 - acc: 0.9674 - binary_accuracy: 0.9674 - val_loss: 1.6355 - val_acc: 0.6500 - val_binary_accuracy: 0.6500\n",
      "Epoch 14/15\n",
      "1073/1073 [==============================] - 3s 3ms/step - loss: 0.0769 - acc: 0.9646 - binary_accuracy: 0.9646 - val_loss: 2.0271 - val_acc: 0.6333 - val_binary_accuracy: 0.6333\n",
      "Epoch 15/15\n",
      "1073/1073 [==============================] - 3s 3ms/step - loss: 0.0812 - acc: 0.9674 - binary_accuracy: 0.9674 - val_loss: 1.6655 - val_acc: 0.6583 - val_binary_accuracy: 0.6583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9aeb4564a8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### split into train test data #################\n",
    "padded_docs_train, padded_docs_test, labels_train, labels_test = train_test_split(padded_docs, labels, test_size=0.2, random_state=69)\n",
    "################### fit the model ##########3#############\n",
    "model.fit(padded_docs_train, labels_train, epochs = 15, validation_split = 0.1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299/299 [==============================] - 0s 405us/step\n",
      "Accuracy: 56.187291\n",
      "binaryAccuracy: 56.187291\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy,binaryAccuracy = model.evaluate(padded_docs_test, labels_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "print('binaryAccuracy: %f' % (binaryAccuracy*100))\n",
    "predicted = model.predict(padded_docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_path = \"../data/AXISBANKTest.csv\"\n",
    "test = pd.read_csv(predict_path)\n",
    "############## prepare tokenizer #################\n",
    "t1 = Tokenizer()\n",
    "t1.fit_on_texts(test)\n",
    "\n",
    "# predict_path = \"../data/AXISBANKTest.csv\"\n",
    "# test = pd.read_csv(predict_path)\n",
    "print(test.size)\n",
    "########## integer encode the documents ##########\n",
    "encoded_test = t.texts_to_sequences(test['title'])\n",
    "print(len(encoded_test))\n",
    "########## pad documents to a max length ##########\n",
    "max_length = 100\n",
    "padded_test = pad_sequences(encoded_test, maxlen=max_length, padding='post')\n",
    "\n",
    "predicted = model.predict(padded_test)\n",
    "# for i in range(predicted.size):\n",
    "#     if predicted[i] < 0.5 :\n",
    "#         predicted[i] = 0.0\n",
    "#     else :\n",
    "#         predicted[i] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidance = np.mean(predicted)\n",
    "if confidance > 0.5:\n",
    "    print (\"buy\")\n",
    "    print(\"confidance is :\",confidance)\n",
    "    \n",
    "else:\n",
    "    if quantity != 0 :\n",
    "        print (\"sell\")\n",
    "        print(\"confidance is :\",1-confidance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
